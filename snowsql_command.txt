#
# Main snowsql command (This will trigger auth thru MFA)
#
snowsql -a dtcipoc.us-east-1 -u chawal

# switch Virtual Warehouse
use warehouse DEMO_WH;

# switch DATABASE
use database "LINEAR_POC_DB";

# switch SCHEMA
use schema LNR_POC:

#
# Confirm the promot to show: chawal#DEMO_WH@LINEAR_POC_DB.LNR_POC
#
select current_warehouse() My_Warehouse, current_database() My_Database, current_schema() My_Schema;



### --- Create Table DDL --- ###

# DDL
create or replace table emp_basic (
first_name string ,
last_name string ,
email string ,
streetaddress string ,
city string ,
start_date date
);

#
# Confirm table structure
#
describe table emp_basic;


### --- LOAD data into STAGE --- ###

# LOAD local CSV files from folder "/Users/chawl001/Dev/scripts/snowflake/test_data/" into TABLE STAGE %emp_basic
put file:///Users/chawl001/Dev/scripts/snowflake/test_data/employees*.csv @LINEAR_POC_DB.LNR_POC.%emp_basic;

# Confirm loaded files in STAGE
list @LINEAR_POC_DB.LNR_POC.%emp_basic;



### --- LOAD data from STAGE into TABLE --- ###

copy into emp_basic
  from @%emp_basic
  file_format = (type = csv field_optionally_enclosed_by='"')
  pattern = '.*employees0[1-5].csv.gz'
  on_error = 'skip_file';

# 
# Confirm table load
#
select count(*) from emp_basic;


### --- Output as CVS --- ###

!set output_format=csv
!set header=true
!spool ./test_output/emp_baisc_tabledata.csv

# Output would be automatically written to CVS also
select * from EMP_BASIC;

!spool off


### --- Unload using Named stage --- ###

# create named stage
CREATE STAGE my_own_staging_area;

SHOW STAGES;

# Copy data into named stage
COPY INTO @my_own_staging_area
  FROM (SELECT * FROM emp_basic LIMIT 15)
  FILE_FORMAT = (TYPE = parquet)
  HEADER = true
  OVERWRITE = true;

# Check the file available in named stage
LIST @my_own_staging_area;

# Download file from Named stage into local file system
get @my_own_staging_area file:///Users/chawl001/Dev/scripts/snowflake/test_output/;


### --- Create external stage backed by Amazon S3 --- ###

CREATE FILE FORMAT my_parquet_format
  TYPE = 'PARQUET'
  COMPRESSION = 'AUTO'
  BINARY_AS_TEXT = TRUE;

create or replace stage my_s3_stage
  url='s3://lineardp-conformed-proposal-dev/test_load_to_snowflake/'
  credentials=(aws_key_id='$AWS_ACCESS_KEY_ID' aws_secret_key='$AWS_SECRET_ACCESS_KEY')
  file_format = (format_name = my_parquet_format);

# Create COPY of original table
CREATE OR REPLACE TABLE EMP_BASIC_PARQUET
  AS SELECT * from EMP_BASIC where 1=0;

describe EMP_BASIC_PARQUET;

COPY INTO EMP_BASIC_PARQUET
  FROM 
  (select
    $1:FIRST_NAME::varchar,
    $1:LAST_NAME::varchar,
    $1:EMAIL::varchar,
    $1:STREETADDRESS::varchar,
    $1:CITY::varchar,
    $1:START_DATE::date
  from @my_s3_stage)
  file_format = (format_name = my_parquet_format);

# Debug for data
select
  $1:FIRST_NAME::varchar FIRST_NAME,
  $1:LAST_NAME::varchar LAST_NAME,
  $1:EMAIL::varchar EMAIL,
  $1:STREETADDRESS::varchar STREETADDRESS,
  $1:CITY::varchar CITY,
  $1:START_DATE::date START_DATE
from @my_s3_stage;

# Load from external S3 stage into internal Table stage
COPY INTO @%emp_basic_parquet
FROM 
  (select
    $1:FIRST_NAME::varchar FIRST_NAME,
    $1:LAST_NAME::varchar LAST_NAME,
    $1:EMAIL::varchar EMAIL,
    $1:STREETADDRESS::varchar STREETADDRESS,
    $1:CITY::varchar CITY,
    $1:START_DATE::date START_DATE
  from @my_s3_stage);

# Load table from internal table stage
COPY INTO emp_basic_parquet
FROM @%emp_basic_parquet;